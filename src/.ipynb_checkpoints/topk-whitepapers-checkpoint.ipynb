{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json, os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Crypto Data\n",
    "- Get data for top 5k crypto by marketcap from coinmarketcap.com\n",
    "- Save data to a file (Prevent usage of Api limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_5k_cryptos() -> pd.DataFrame:\n",
    "    \n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "    url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest'\n",
    "\n",
    "    headers = {\n",
    "        'Accepts': 'application/json',\n",
    "        'X-CMC_PRO_API_KEY': api_key,\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'start': '1',\n",
    "        'limit': '5000',  # You can adjust this to get more or fewer listings\n",
    "        'convert': 'USD',\n",
    "        'sort': 'market_cap'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        df = pd.DataFrame(data['data'])\n",
    "        return df\n",
    "        print(f'Data Saved..')\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {data['status']['error_message']}\")\n",
    "        \n",
    "# df = get_top_5k_cryptos()\n",
    "# df['whitepaper_link'] = ''\n",
    "# df.to_csv('../data/topk_crypto.csv', index=False)\n",
    "# print('Collected top 5k crypto.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../data/topk_crypto_w_whitepaper_link.csv')\n",
    "print(f'Dimensions of the dataset: {df.shape}')\n",
    "display(df.sample(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get whitepaper links from cmc  for each of the topk cryptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whitepaper_link(slug):\n",
    "    url = f'https://coinmarketcap.com/currencies/{slug}/'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    divs = soup.find_all('div', class_='sc-d1ede7e3-0 sc-7f0f401-0 gRSwoF gQoblf')\n",
    "    \n",
    "    for div in divs:\n",
    "        if 'whitepaper' not in div.get_text().lower():\n",
    "            continue\n",
    "        \n",
    "        # Find the whitepaper link within this div\n",
    "        for a_tag in div.find_all('a', href=True):\n",
    "            return a_tag['href']\n",
    "    \n",
    "    return None\n",
    "\n",
    "tqdm.pandas()\n",
    "# df['whitepaper_cmc_link'] = df['whitepaper_link'].apply(lambda x: x if pd.notnull(x) and str(x).strip() else None)\n",
    "# df['whitepaper_cmc_link'] = df.progress_apply(lambda row: get_whitepaper_link(row['slug']) if pd.isnull(row['whitepaper_link']) else row['whitepaper_link'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get whitepaper links from whitepaper.io for each of the topk cryptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whitepaper_pdf_link(slug):\n",
    "    try:\n",
    "        # Step 1: Make the first API call to get the coin ID\n",
    "        url_1 = f'https://api-new.whitepaper.io/coins?slug={slug}'\n",
    "        response_1 = requests.get(url_1)\n",
    "        response_1.raise_for_status()\n",
    "        \n",
    "        data_1 = response_1.json()\n",
    "        if '_id' in data_1:\n",
    "            coin_id = data_1['_id']\n",
    "        else:\n",
    "            print(\"ID attribute not found in the coin response\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch coin data: {e}\")\n",
    "        return None\n",
    "    except ValueError:\n",
    "        print(\"Failed to parse JSON response for coin data\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Step 2: Make the subsequent API call to get the document key\n",
    "        url_2 = f'https://api-new.whitepaper.io/documents?id={coin_id}'\n",
    "        response_2 = requests.get(url_2)\n",
    "        response_2.raise_for_status()\n",
    "\n",
    "        data_2 = response_2.json()\n",
    "        if data_2 and isinstance(data_2, list):\n",
    "            pdf_data = data_2[0]\n",
    "            if 'key' in pdf_data:\n",
    "                document_key = pdf_data['key']\n",
    "            else:\n",
    "                print(\"Key attribute not found in the document response\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"Document data is not in the expected format\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to fetch document data: {e}\")\n",
    "        return None\n",
    "    except ValueError:\n",
    "        print(\"Failed to parse JSON response for document data\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Step 3: Construct the PDF link\n",
    "        pdf_link = f'https://api-new.whitepaper.io/documents/pdf?id={document_key}'\n",
    "        return pdf_link\n",
    "    except Exception as e:\n",
    "        print(f\"Error constructing PDF link: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['whitepaper_io_link'][:500] = df[:500].progress_apply(lambda row: row['whitepaper_io_link'] if pd.notnull(row['whitepaper_io_link']) and str(row['whitepaper_io_link']).strip() else get_whitepaper_pdf_link(row['slug']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['whitepaper_io_link'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'../data/topk_crypto_w_whitepaper_link.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset with whitepaper links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/topk_crypto_w_whitepaper_link.csv')\n",
    "extend_df = pd.read_csv('../data/whitepaper_link_corrections.csv')\n",
    "\n",
    "\n",
    "df = df.merge(extend_df, on='slug', how='left', suffixes=('', '_extend'))\n",
    "# Override the 'whitepaper_link' column with the values from 'whitepaper_link_extend'\n",
    "df['whitepaper_link'] = df['whitepaper_link_extend'].combine_first(df['whitepaper_link'])\n",
    "# Drop the extended 'whitepaper_link' column\n",
    "df = df.drop(columns=['whitepaper_link_extend'])\n",
    "\n",
    "df = df.drop(columns = ['tags', 'quote'])\n",
    "df = df[df['whitepaper_link'].notna()]\n",
    "print(f'Shape of dataframe: ', df.shape)\n",
    "print(f'Number of whitepaper links: ', df.whitepaper_link.nunique())\n",
    "display(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify PDF structure\n",
    "def verify_pdf(file_name):\n",
    "    try:\n",
    "        with open(file_name, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            # Check if the PDF has at least one page\n",
    "            if len(reader.pages) > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred while verifying {file_name}. Error: {e}')\n",
    "        # traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "# Function to download PDF\n",
    "def fetch_webpage_content(url, file_name):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            content = response.content\n",
    "            \n",
    "            if 'pdf' in file_name:\n",
    "                with open(file_name, 'wb') as file:\n",
    "                    file.write(content)\n",
    "                    \n",
    "                if not verify_pdf(file_name):\n",
    "                    invalid_file_name = file_name.replace('.pdf', '_invalid.pdf')\n",
    "                    os.rename(file_name, invalid_file_name)\n",
    "                \n",
    "            else:\n",
    "                soup = BeautifulSoup(content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "                content = soup.get_text(separator='\\n')\n",
    "                \n",
    "                with open(file_name, 'w') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "        else:\n",
    "            print(f'Failed to download {file_name}, with url: {url}')\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred while downloading {file_name}, with url: {url}, Error is: {e}')\n",
    "        \n",
    "        \n",
    "\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    url = row['whitepaper_link']\n",
    "    type = 'pdf' if 'pdf' in url else 'txt'\n",
    "    file_name = f\"../data/whitepapers/{index + 1}_{row['slug']}.{type}\"\n",
    "    \n",
    "    if type == 'txt' and os.path.exists(file_name):\n",
    "        os.remove(file_name)\n",
    "    \n",
    "    if not os.path.exists(file_name):\n",
    "        fetch_webpage_content(url, file_name)\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
